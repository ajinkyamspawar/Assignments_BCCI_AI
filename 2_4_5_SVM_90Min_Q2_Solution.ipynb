{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e20b448",
   "metadata": {},
   "source": [
    "                                                LAB ASSIGNMENTS\n",
    "                                        Module 2: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddd369",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed464950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753a1dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pulsar_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpulsar_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# view dimensions of dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pulsar_data.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = 'pulsar_data.csv'\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# view dimensions of dataset\n",
    "\n",
    "df.shape\n",
    "\n",
    "\"\"\"We can see that there are 17898 instances and 9 feature variables in the data set.\"\"\"\n",
    "\n",
    "# let's preview the dataset\n",
    "\n",
    "df.head()\n",
    "\n",
    "\"\"\"We can see that there are 9 variables in the dataset. 8 are continuous variables and 1 is discrete variable. The discrete variable is `target_class` variable. It is also the target variable.\n",
    "\n",
    "\n",
    "Now, I will view the column names to check for leading and trailing spaces.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the column names of the dataframe\n",
    "\n",
    "col_names = df.columns\n",
    "\n",
    "col_names\n",
    "\n",
    "\n",
    "\n",
    "# remove leading spaces from column names\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# view column names again\n",
    "\n",
    "df.columns\n",
    "\n",
    "\n",
    "# rename column names\n",
    "\n",
    "df.columns = ['IP Mean', 'IP Sd', 'IP Kurtosis', 'IP Skewness', \n",
    "              'DM-SNR Mean', 'DM-SNR Sd', 'DM-SNR Kurtosis', 'DM-SNR Skewness', 'target_class']\n",
    "\n",
    "# view the renamed column names\n",
    "\n",
    "df.columns\n",
    "\n",
    "\n",
    "\n",
    "# check distribution of target_class column\n",
    "\n",
    "df['target_class'].value_counts()\n",
    "\n",
    "# view the percentage distribution of target_class column\n",
    "\n",
    "df['target_class'].value_counts()/np.float(len(df))\n",
    "\n",
    "df.info()\n",
    "\n",
    "\"\"\"### Explore missing values in variables\"\"\"\n",
    "\n",
    "# check for missing values in variables\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "# view summary statistics in numerical variables\n",
    "\n",
    "round(df.describe(),2)\n",
    "\n",
    "\"\"\"## Declaring feature vector and target variable\"\"\"\n",
    "\n",
    "X = df.drop(['target_class'], axis=1)\n",
    "\n",
    "y = df['target_class']\n",
    "\n",
    "\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\"\"\"## Feature Scaling\"\"\"\n",
    "\n",
    "cols = X_train.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "\n",
    "X_train.describe()\n",
    "\n",
    "\n",
    "\n",
    "# import SVC classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import metrics to compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# instantiate classifier with default hyperparameters\n",
    "svc=SVC() \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "# instantiate classifier with rbf kernel and C=100\n",
    "svc=SVC(C=100.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# instantiate classifier with rbf kernel and C=1000\n",
    "svc=SVC(C=1000.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "# instantiate classifier with linear kernel and C=1.0\n",
    "linear_svc=SVC(kernel='linear', C=1.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "linear_svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_test=linear_svc.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))\n",
    "\n",
    "\n",
    "\n",
    "# instantiate classifier with linear kernel and C=100.0\n",
    "linear_svc100=SVC(kernel='linear', C=100.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "linear_svc100.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=linear_svc100.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with linear kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "# instantiate classifier with linear kernel and C=1000.0\n",
    "linear_svc1000=SVC(kernel='linear', C=1000.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "linear_svc1000.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=linear_svc1000.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with linear kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "y_pred_train = linear_svc.predict(X_train)\n",
    "\n",
    "y_pred_train\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for overfitting and underfitting\n",
    "\n",
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(linear_svc.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(linear_svc.score(X_test, y_test)))\n",
    "\n",
    "\"\"\"The training-set accuracy score is 0.9783 while the test-set accuracy to be 0.9830. These two values are quite comparable. So, there is no question of overfitting.\n",
    "\n",
    "## Run SVM with polynomial kernel\n",
    "\n",
    "\n",
    "### Run SVM with polynomial kernel and C=1.0\n",
    "\"\"\"\n",
    "\n",
    "# instantiate classifier with polynomial kernel and C=1.0\n",
    "poly_svc=SVC(kernel='poly', C=1.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "poly_svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=poly_svc.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\"\"\" ### Run SVM with polynomial kernel and C=100.0\"\"\"\n",
    "\n",
    "# instantiate classifier with polynomial kernel and C=100.0\n",
    "poly_svc100=SVC(kernel='poly', C=100.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "poly_svc100.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=poly_svc100.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\"\"\"Polynomial kernel gives poor performance. It may be overfitting the training set.\n",
    "\n",
    "## Run SVM with sigmoid kernel\n",
    "\n",
    "\n",
    "### Run SVM with sigmoid kernel and C=1.0\n",
    "\"\"\"\n",
    "\n",
    "# instantiate classifier with sigmoid kernel and C=1.0\n",
    "sigmoid_svc=SVC(kernel='sigmoid', C=1.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "sigmoid_svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=sigmoid_svc.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\"\"\"### Run SVM with sigmoid kernel and C=100.0\"\"\"\n",
    "\n",
    "# instantiate classifier with sigmoid kernel and C=100.0\n",
    "sigmoid_svc100=SVC(kernel='sigmoid', C=100.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "sigmoid_svc100.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=sigmoid_svc100.predict(X_test)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with sigmoid kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\"\"\"We can see that sigmoid kernel is also performing poorly just like with polynomial kernel.\n",
    "\n",
    "### Comments\n",
    "\n",
    "\n",
    "We get maximum accuracy with `rbf` and `linear` kernel with C=100.0. and the accuracy is 0.9832. Based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n",
    "\n",
    "\n",
    "But, this is not true. Here, we have an imbalanced dataset. The problem is that accuracy is an inadequate measure for quantifying predictive performance in the imbalanced dataset problem.\n",
    "\n",
    "\n",
    "So, we must explore alternative metrices that provide better guidance in selecting models. In particular, we would like to know the underlying distribution of values and the type of errors our classifer is making. \n",
    "\n",
    "\n",
    "One such metric to analyze the model performance in imbalanced classes problem is `Confusion matrix`.\n",
    "\"\"\"\n",
    "\n",
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])\n",
    "\n",
    "\"\"\"The confusion matrix shows `3289 + 230 = 3519 correct predictions` and `17 + 44 = 61 incorrect predictions`.\n",
    "\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "\n",
    "- `True Positives` (Actual Positive:1 and Predict Positive:1) - 3289\n",
    "\n",
    "\n",
    "- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 230\n",
    "\n",
    "\n",
    "- `False Positives` (Actual Negative:0 but Predict Positive:1) - 17 `(Type I error)`\n",
    "\n",
    "\n",
    "- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 44 `(Type II error)`\n",
    "\"\"\"\n",
    "\n",
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "\n",
    "\"\"\"## Classification metrics\"\"\"\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "\"\"\"### Classification accuracy\"\"\"\n",
    "\n",
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "# print classification accuracy\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n",
    "\n",
    "\"\"\"### Classification error\"\"\"\n",
    "\n",
    "# print classification error\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))\n",
    "\n",
    "\"\"\"### Precision\n",
    "\n",
    "\n",
    "**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). \n",
    "\n",
    "\n",
    "So, **Precision** identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
    "\n",
    "\n",
    "\n",
    "Mathematically, precision can be defined as the ratio of `TP to (TP + FP)`.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# print precision score\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "\n",
    "\"\"\"### Recall\n",
    "\n",
    "\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.\n",
    "It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.\n",
    "\n",
    "\n",
    "**Recall** identifies the proportion of correctly predicted actual positives.\n",
    "\n",
    "\n",
    "Mathematically, **recall** can be defined as the ratio of `TP to (TP + FN)`.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))\n",
    "\n",
    "\"\"\"### True Positive Rate\n",
    "\n",
    "\n",
    "**True Positive Rate** is synonymous with **Recall**.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "true_positive_rate = TP / float(TP + FN)\n",
    "\n",
    "\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n",
    "\n",
    "\"\"\"### False Positive Rate\"\"\"\n",
    "\n",
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n",
    "\n",
    "\"\"\"### Specificity\"\"\"\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "\n",
    "\"\"\"### f1-score\n",
    "\n",
    "\n",
    "**f1-score** is the weighted harmonic mean of precision and recall. The best possible **f1-score** would be 1.0 and the worst \n",
    "would be 0.0.  **f1-score** is the harmonic mean of precision and recall. So, **f1-score** is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of `f1-score` should be used to \n",
    "compare classifier models, not global accuracy.\n",
    "\n",
    "### Support\n",
    "\n",
    "\n",
    "**Support** is the actual number of occurrences of the class in our dataset.\n",
    "\"\"\"\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for Predicting a Pulsar Star classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.\n",
    "\n",
    "### ROC  AUC\n",
    "\n",
    "\n",
    "**ROC AUC** stands for **Receiver Operating Characteristic - Area Under Curve**. It is a technique to compare classifier performance. In this technique, we measure the `area under the curve (AUC)`. A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. \n",
    "\n",
    "\n",
    "So, **ROC AUC** is the percentage of the ROC plot that is underneath the curve.\n",
    "\"\"\"\n",
    "\n",
    "# compute ROC AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))\n",
    "\n",
    "\"\"\"### Comments\n",
    "\n",
    "\n",
    "- ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n",
    "\n",
    "- ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in classifying the pulsar star.\n",
    "\"\"\"\n",
    "\n",
    "# calculate cross-validated ROC AUC \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Cross_validated_ROC_AUC = cross_val_score(linear_svc, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "\n",
    "print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))\n",
    "\n",
    "\"\"\"### Stratified k-Fold Cross Validation with shuffle split with  linear kernel\"\"\"\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "linear_svc=SVC(kernel='linear')\n",
    "\n",
    "\n",
    "linear_scores = cross_val_score(linear_svc, X, y, cv=kfold)\n",
    "\n",
    "# print cross-validation scores with linear kernel\n",
    "\n",
    "print('Stratified cross-validation scores with linear kernel:\\n\\n{}'.format(linear_scores))\n",
    "\n",
    "# print average cross-validation score with linear kernel\n",
    "\n",
    "print('Average stratified cross-validation score with linear kernel:{:.4f}'.format(linear_scores.mean()))\n",
    "\n",
    "\"\"\"### Stratified k-Fold Cross Validation with shuffle split with rbf kernel\"\"\"\n",
    "\n",
    "rbf_svc=SVC(kernel='rbf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea318c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_scores = cross_val_score(rbf_svc, X, y, cv=kfold)\n",
    "\n",
    "# print cross-validation scores with rbf kernel\n",
    "\n",
    "print('Stratified Cross-validation scores with rbf kernel:\\n\\n{}'.format(rbf_scores))\n",
    "\n",
    "# print average cross-validation score with rbf kernel\n",
    "\n",
    "print('Average stratified cross-validation score with rbf kernel:{:.4f}'.format(rbf_scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
